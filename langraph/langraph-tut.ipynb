{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b150a4f",
   "metadata": {},
   "source": [
    "## https://langchain-ai.github.io/langgraph/agents/agents/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a256f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langgraph \"langchain[anthropic]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e5f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0544c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:\\PROGRAMMING PROJECTS\\langchain_crash_course\\RestaurantNameGenerator\\myenv\n"
     ]
    }
   ],
   "source": [
    "# to check the current Python environment\n",
    "import sys\n",
    "print(sys.prefix)\n",
    "\n",
    "import os\n",
    "print(\"LANGCHAIN_TRACING_V2:\", os.environ.get(\"LANGCHAIN_TRACING_V2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740794a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from .env into environment\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30369471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "assert \"ANTHROPIC_API_KEY\" in os.environ\n",
    "assert \"OPENAI_API_KEY\" in os.environ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5255276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f66f3b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='294', name='multiply', tool_call_id='1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(multiply.invoke({\"a\": 6, \"b\": 7}))  # returns 42\n",
    "tool_call = {\n",
    "    \"type\": \"tool_call\",\n",
    "    \"id\": \"1\",\n",
    "    \"args\": {\"a\": 42, \"b\": 7}\n",
    "}\n",
    "multiply.invoke(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "269f1a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='how is 3.14 x 7?', additional_kwargs={}, response_metadata={}, id='003a4140-3b23-43d5-ae5c-5e2897d5041d'),\n",
      "              AIMessage(content=[{'id': 'toolu_01F5Eq8Y4K7UZjdvhpSoEBep', 'input': {'a': 3.14, 'b': 7}, 'name': 'multiply', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01W2baqsF6so4Ld7rapoJ7gJ', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 405, 'output_tokens': 70, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-haiku-20240307'}, id='run--3e67e4b8-9c60-4c9d-9fca-e18d6e30ea7b-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3.14, 'b': 7}, 'id': 'toolu_01F5Eq8Y4K7UZjdvhpSoEBep', 'type': 'tool_call'}], usage_metadata={'input_tokens': 405, 'output_tokens': 70, 'total_tokens': 475, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}),\n",
      "              ToolMessage(content='21.98', name='multiply', id='ac8219c4-53ae-48a8-97aa-95d068824794', tool_call_id='toolu_01F5Eq8Y4K7UZjdvhpSoEBep'),\n",
      "              AIMessage(content='The result of multiplying 3.14 by 7 is 21.98.', additional_kwargs={}, response_metadata={'id': 'msg_0112uZz44dQPAyE5yyaDqy5d', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 490, 'output_tokens': 24, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-haiku-20240307'}, id='run--0a1f19bd-3983-4004-b1d8-5a692f50b474-0', usage_metadata={'input_tokens': 490, 'output_tokens': 24, 'total_tokens': 514, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.tools import tool\n",
    "from pprint import pprint\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def get_weather(city: str) -> str:  \n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "model = ChatAnthropic(\n",
    "    # model='claude-sonnet-4-20250514',\n",
    "    model='claude-3-haiku-20240307',\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "agent = create_react_agent(\n",
    "    # model='openai:gpt-4o',\n",
    "    # model='claude-3-5-haiku-latest', # this is the cheapest model available in Anthropic\n",
    "    # model='claude-sonnet-4-20250514',\n",
    "    model=model,\n",
    "    tools=[multiply, get_weather],\n",
    "    prompt=\"You are a helpful assistant\"\n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    # {\"messages\": [{\"role\": \"user\", \"content\": \"how is climate in berlin?\"}]}\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"how is 3.14 x 7?\"}]}\n",
    ")\n",
    "pprint(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119cfa9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langgraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# static prompt that never changes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprebuilt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_react_agent\n\u001b[32m      5\u001b[39m agent = create_react_agent(\n\u001b[32m      6\u001b[39m     model=model,\n\u001b[32m      7\u001b[39m     tools=[get_weather],\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# A static prompt that never changes\u001b[39;00m\n\u001b[32m      9\u001b[39m     prompt=\u001b[33m\"\u001b[39m\u001b[33mNever answer questions about the weather.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m agent.invoke(\n\u001b[32m     13\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mwhat is 5x5\u001b[39m\u001b[33m\"\u001b[39m}]}\n\u001b[32m     14\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langgraph'"
     ]
    }
   ],
   "source": [
    "# static prompt that never changes\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[get_weather],\n",
    "    # A static prompt that never changes\n",
    "    prompt=\"Never answer questions about the weather.\"\n",
    ")\n",
    "\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is 5x5\"}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab77eb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='6d83f6ff-9cc2-4626-9c20-90bbeab67c17'),\n",
       "  AIMessage(content=[{'text': 'Okay, John Smith, let me check the weather for you in SF:', 'type': 'text'}, {'id': 'toolu_013dso225d8f1b9sVrcE8hS8', 'input': {'city': 'sf'}, 'name': 'get_weather', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01Ly96EeYthgLXKC8anZqWkr', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 345, 'output_tokens': 70, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-haiku-20240307'}, id='run--1083f3f2-b6f8-4967-9563-d23042df8753-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'sf'}, 'id': 'toolu_013dso225d8f1b9sVrcE8hS8', 'type': 'tool_call'}], usage_metadata={'input_tokens': 345, 'output_tokens': 70, 'total_tokens': 415, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}),\n",
       "  ToolMessage(content=\"It's always sunny in sf!\", name='get_weather', id='103507a2-0702-4d7f-8307-40afdc7ce4e7', tool_call_id='toolu_013dso225d8f1b9sVrcE8hS8'),\n",
       "  AIMessage(content='The weather in San Francisco is sunny today. Let me know if you need anything else!', additional_kwargs={}, response_metadata={'id': 'msg_01TfKBh1NspmVVrMaBGvmDEU', 'model': 'claude-3-haiku-20240307', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 432, 'output_tokens': 22, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-haiku-20240307'}, id='run--1c5925da-3434-414f-ba3c-b986299da6c6-0', usage_metadata={'input_tokens': 432, 'output_tokens': 22, 'total_tokens': 454, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dynamic prompt that changes based on the input\n",
    "\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]:  \n",
    "    user_name = config[\"configurable\"].get(\"user_name\")\n",
    "    system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\"\n",
    "    return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[get_weather],\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n",
    "    config={\"configurable\": {\"user_name\": \"John Smith\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfdff969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory to the agent\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model,\n",
    "    tools=[get_weather],\n",
    "    checkpointer=checkpointer  \n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "sf_response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n",
    "    config  \n",
    ")\n",
    "ny_response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]},\n",
    "    config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d25f90bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeatherResponse(conditions='Sunny and mild')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Configure structured output\n",
    "# To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "class WeatherResponse(BaseModel):\n",
    "    conditions: str\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model,\n",
    "    tools=[get_weather],\n",
    "    response_format=WeatherResponse  \n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")\n",
    "\n",
    "response[\"structured_response\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3eb6072f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154de92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
